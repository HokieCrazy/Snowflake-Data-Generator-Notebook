{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dynamic Data Generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNNZaN+X/esh9Srci5JQTUy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HokieCrazy/Snowflake-Data-Generator-Notebook/blob/master/Dynamic_Data_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMER2B5R-h5A",
        "colab_type": "text"
      },
      "source": [
        "## Install Libraries using pip\n",
        "This must be done when using Colab Notebooks, since the instance of Python is not a permanent one.  Once all of the pip installs are done, the `os._exit(00)` line should be executed to reset the kernel for you.  You will receive an error message, but this is expected.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKuXOXDLeyto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade snowflake-connector-python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CC6KlPqfVxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFzjTPtufgd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade snowflake-sqlalchemy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK4jDkCcxCv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os._exit(00)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bddz7BgK_E52"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uL--XWAeS9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import base64\n",
        "import pandas as pd\n",
        "from snowflake.sqlalchemy import URL\n",
        "import sqlalchemy as sa\n",
        "from sys import exit\n",
        "from sqlalchemy import text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hy75l3ya_K90"
      },
      "source": [
        "## Set your Snowflake Environments\n",
        "This Notebook allows to read from a Source account, database, and schema.  Once the information is read, the data can be generated in a separate account, database, and schema.\n",
        "\n",
        "##### *Note: This does not mean the Notebook is copying any data from Source to Target.*\n",
        "\n",
        "##### *Note: The sourcePassword and targetPassword entered into the page should be the base64 encrypted password, not the plain text password.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn7n8sVhu7Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Source System to gather Statistics from\n",
        "sourceUser = ''\n",
        "sourceAccount = ''\n",
        "sourcePassword = ''\n",
        "sourceDatabase = ''\n",
        "sourceSchema = ''\n",
        "sourceWarehouse = ''\n",
        "\n",
        "# Target System to generate data to\n",
        "targetUser = ''\n",
        "targetAccount = ''\n",
        "targetPassword = ''\n",
        "targetDatabase = ''\n",
        "targetSchema = ''\n",
        "targetWarehouse = ''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fEVzjlMaABUu"
      },
      "source": [
        "## Compile Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBEWCn13fGUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getEngine(snowUser=None, snowPassword=None, snowAccount=None, snowDatabase=None, snowSchema=None, snowWarehouse=None):\n",
        "    try:\n",
        "        base64_bytes = snowPassword.encode('ascii')\n",
        "        message_bytes = base64.b64decode(base64_bytes)\n",
        "        password = message_bytes.decode('ascii')\n",
        "\n",
        "        SnowEngine = sa.create_engine(URL(\n",
        "            user=snowUser,\n",
        "            password=password,\n",
        "            account=snowAccount,\n",
        "            database=snowDatabase,\n",
        "            schema=snowSchema,\n",
        "            warehouse=snowWarehouse\n",
        "        ))\n",
        "\n",
        "        return SnowEngine\n",
        "\n",
        "    except Exception as e:\n",
        "        errorStr = 'ERROR: ' + str(e)\n",
        "        return errorStr\n",
        "\n",
        "def execSQLDataFrame(SnowEngine, sqlQuery):\n",
        "    try:\n",
        "        if SnowEngine is None:\n",
        "            print('SnowEngine argument is required')\n",
        "            exit(InvalidArgsCode)\n",
        "        if sqlQuery is None:\n",
        "            print('sqlQuery argument is required')\n",
        "            exit(InvalidArgsCode)\n",
        "\n",
        "        resultSet = pd.read_sql(text(sqlQuery), SnowEngine)\n",
        " \n",
        "        return resultSet\n",
        "\n",
        "    except Exception as e:\n",
        "        errorStr = 'ERROR (execLoadDBQuery)' + str(e)\n",
        "        print(errorStr)\n",
        "        raise\n",
        "\n",
        "def execColStats(dfColumns, rows):\n",
        "    for i,row in dfColumns.iterrows():\n",
        "        if dfColumns.data_type[i] == 'TEXT':\n",
        "            sqlQuery = \"\"\"\n",
        "                SELECT '\"\"\" + str(dfColumns.table_name[i]) + \"\"\"' as table_name,\n",
        "                        '\"\"\" + str(dfColumns.column_name[i]) + \"\"\"' as column_name,\n",
        "                        '\"\"\" + str(dfColumns.data_type[i]) + \"\"\"' as data_type,\n",
        "                        \"\"\" + str(dfColumns.character_maximum_length[i]) + \"\"\"::NUMBER as data_type_length,\n",
        "                        NULL::NUMBER as data_type_precision,\n",
        "                        MIN(LENGTH(\"\"\" + str(dfColumns.column_name[i]) + \"\"\"))::NUMBER as min_length,\n",
        "                        MAX(LENGTH(\"\"\" + str(dfColumns.column_name[i]) + \"\"\"))::NUMBER as max_length,\n",
        "                        COUNT(DISTINCT \"\"\" + str(dfColumns.column_name[i]) + \"\"\")::NUMBER as cardinality,\n",
        "                        \"\"\" + rows + \"\"\"::NUMBER as total_cardinality,\n",
        "                        SUM(CASE WHEN \"\"\" + str(dfColumns.column_name[i]) + \"\"\" IS NULL THEN 1 ELSE 0 END)::NUMBER as total_null\n",
        "                FROM \"\"\" + str(dfColumns.table_name[i]) + \"\"\";\"\"\"\n",
        "        elif dfColumns.data_type[i] == 'FLOAT':\n",
        "            sqlQuery = \"\"\"\n",
        "                SELECT '\"\"\" + str(dfColumns.table_name[i]) + \"\"\"' as table_name,\n",
        "                        '\"\"\" + str(dfColumns.column_name[i]) + \"\"\"' as column_name,\n",
        "                        '\"\"\" + str(dfColumns.data_type[i]) + \"\"\"' as data_type,\n",
        "                        NULL::NUMBER as data_type_length,\n",
        "                        NULL::NUMBER as data_type_precision,\n",
        "                        MIN((\"\"\" + str(dfColumns.column_name[i]) + \"\"\"))::NUMBER as min_length,\n",
        "                        MAX((\"\"\" + str(dfColumns.column_name[i]) + \"\"\"))::NUMBER as max_length,\n",
        "                        COUNT(DISTINCT \"\"\" + str(dfColumns.column_name[i]) + \"\"\")::NUMBER as cardinality,\n",
        "                        \"\"\" + rows + \"\"\"::NUMBER as total_cardinality,\n",
        "                        SUM(CASE WHEN \"\"\" + str(dfColumns.column_name[i]) + \"\"\" IS NULL THEN 1 ELSE 0 END)::NUMBER as total_null\n",
        "                FROM \"\"\" + str(dfColumns.table_name[i]) + \"\"\";\"\"\"\n",
        "        elif dfColumns.data_type[i] == 'NUMBER':\n",
        "            sqlQuery = \"\"\"\n",
        "                SELECT '\"\"\" + str(dfColumns.table_name[i]) + \"\"\"' as table_name,\n",
        "                        '\"\"\" + str(dfColumns.column_name[i]) + \"\"\"' as column_name,\n",
        "                        '\"\"\" + str(dfColumns.data_type[i]) + \"\"\"' as data_type,\n",
        "                        \"\"\" + str(dfColumns.numeric_precision[i]) + \"\"\"::NUMBER as data_type_length,\n",
        "                        \"\"\" + str(dfColumns.numeric_scale[i]) + \"\"\"::NUMBER as data_type_precision,\n",
        "                        MIN((\"\"\" + str(dfColumns.column_name[i]) + \"\"\"))::NUMBER as min_length,\n",
        "                        MAX((\"\"\" + str(dfColumns.column_name[i]) + \"\"\"))::NUMBER as max_length,\n",
        "                        COUNT(DISTINCT \"\"\" + str(dfColumns.column_name[i]) + \"\"\")::NUMBER as cardinality,\n",
        "                        \"\"\" + rows + \"\"\"::NUMBER as total_cardinality,\n",
        "                        SUM(CASE WHEN \"\"\" + str(dfColumns.column_name[i]) + \"\"\" IS NULL THEN 1 ELSE 0 END)::NUMBER as total_null\n",
        "                FROM \"\"\" + str(dfColumns.table_name[i]) + \"\"\";\"\"\"\n",
        "        elif dfColumns.data_type[i] == 'DATE' or dfColumns.data_type[i] == 'TIMESTAMP' or dfColumns.data_type[i] == 'TIMESTAMP_LTZ' or dfColumns.data_type[i] == 'TIMESTAMP_NTZ':\n",
        "            sqlQuery = \"\"\"\n",
        "                SELECT '\"\"\" + str(dfColumns.table_name[i]) + \"\"\"' as table_name,\n",
        "                        '\"\"\" + str(dfColumns.column_name[i]) + \"\"\"' as column_name,\n",
        "                        '\"\"\" + str(dfColumns.data_type[i]) + \"\"\"' as data_type,\n",
        "                        NULL::NUMBER as data_type_length,\n",
        "                        NULL::NUMBER as data_type_precision,\n",
        "                        NULL::NUMBER as min_length,\n",
        "                        NULL::NUMBER as max_length,\n",
        "                        COUNT(DISTINCT \"\"\" + str(dfColumns.column_name[i]) + \"\"\")::NUMBER as cardinality,\n",
        "                        \"\"\" + rows + \"\"\"::NUMBER as total_cardinality,\n",
        "                        SUM(CASE WHEN \"\"\" + str(dfColumns.column_name[i]) + \"\"\" IS NULL THEN 1 ELSE 0 END)::NUMBER as total_null\n",
        "                FROM \"\"\" + str(dfColumns.table_name[i]) + \"\"\";\"\"\"\n",
        "        elif dfColumns.data_type[i] == 'ARRAY':\n",
        "            sqlQuery = \"\"\"\n",
        "                SELECT '\"\"\" + str(dfColumns.table_name[i]) + \"\"\"' as table_name,\n",
        "                        '\"\"\" + str(dfColumns.column_name[i]) + \"\"\"' as column_name,\n",
        "                        '\"\"\" + str(dfColumns.data_type[i]) + \"\"\"' as data_type,\n",
        "                        NULL::NUMBER as data_type_length,\n",
        "                        NULL::NUMBER as data_type_precision,\n",
        "                        MIN(ARRAY_SIZE(\"\"\" + str(dfColumns.column_name[i]) + \"\"\"))::NUMBER as min_length,\n",
        "                        MAX(ARRAY_SIZE(\"\"\" + str(dfColumns.column_name[i]) + \"\"\"))::NUMBER as max_length,\n",
        "                        COUNT(DISTINCT \"\"\" + str(dfColumns.column_name[i]) + \"\"\")::NUMBER as cardinality,\n",
        "                        \"\"\" + rows + \"\"\"::NUMBER as total_cardinality,\n",
        "                        SUM(CASE WHEN \"\"\" + str(dfColumns.column_name[i]) + \"\"\" IS NULL THEN 1 ELSE 0 END)::NUMBER as total_null\n",
        "                FROM \"\"\" + str(dfColumns.table_name[i]) + \"\"\";\"\"\"\n",
        "        elif dfColumns.data_type[i] != 'VARIANT':\n",
        "            sqlQuery = \"\"\"\n",
        "                SELECT '\"\"\" + str(dfColumns.table_name[i]) + \"\"\"' as table_name,\n",
        "                        '\"\"\" + str(dfColumns.column_name[i]) + \"\"\"' as column_name,\n",
        "                        '\"\"\" + str(dfColumns.data_type[i]) + \"\"\"' as data_type,\n",
        "                        NULL::NUMBER as data_type_length,\n",
        "                        NULL::NUMBER as data_type_precision,\n",
        "                        MIN(LENGTH(\"\"\" + str(dfColumns.column_name[i]) + \"\"\"))::NUMBER as min_length,\n",
        "                        MAX(LENGTH(\"\"\" + str(dfColumns.column_name[i]) + \"\"\"))::NUMBER as max_length,\n",
        "                        COUNT(DISTINCT \"\"\" + str(dfColumns.column_name[i]) + \"\"\")::NUMBER as cardinality,\n",
        "                        \"\"\" + rows + \"\"\"::NUMBER as total_cardinality,\n",
        "                        SUM(CASE WHEN \"\"\" + str(dfColumns.column_name[i]) + \"\"\" IS NULL THEN 1 ELSE 0 END)::NUMBER as total_null\n",
        "                FROM \"\"\" + str(dfColumns.table_name[i]) + \"\"\";\"\"\"\n",
        "\n",
        "        if i==0:\n",
        "            dfResult = execSQLDataFrame(SourceEngine, sqlQuery)\n",
        "        else:\n",
        "            dfTemp = execSQLDataFrame(SourceEngine, sqlQuery)\n",
        "            dfResult = dfResult.append(dfTemp, ignore_index = True)\n",
        "\n",
        "    return dfResult\n",
        "\n",
        "def execGenDataSQL(dfStats):\n",
        "    \n",
        "    seed_1 = 10000  #default seed value, increaments for each row added\n",
        "\n",
        "    sqlQuery = \"\"\"\n",
        "        INSERT INTO \"\"\" + dfStats.table_name[0] + \"\"\"(\"\"\"\n",
        "    \n",
        "    for i,row in dfStats.iterrows():\n",
        "        if i == 0:\n",
        "            sqlQuery += dfStats.column_name[i]\n",
        "        else:\n",
        "            sqlQuery += \"\"\", \"\"\" + dfStats.column_name[i]\n",
        "    \n",
        "    sqlQuery += \"\"\")\n",
        "        SELECT \"\"\"\n",
        "\n",
        "    for i,row in dfStats.iterrows():\n",
        "        if dfStats.data_type[i].upper() == 'DATE':\n",
        "            fb = 'dateadd(day, -uniform(1, ' + str(dfStats.cardinality[i]) + ', random('+ str(seed_1) +')), date_trunc(day, current_date)) '\n",
        "            fe = '::date '\n",
        "        elif dfStats.data_type[i].upper() == 'TIMESTAMP' or dfStats.data_type[i].upper() == 'TIMESTAMP_LTZ' or dfStats.data_type[i].upper() == 'TIMESTAMP_NTZ':\n",
        "            fb = '(date_part(epoch_second, current_date) - (uniform(1, ' + str(dfStats.cardinality[i]) + ', random('+ str(seed_1) +')))) '\n",
        "            fe = '::timestamp '\n",
        "        elif dfStats.data_type[i].upper() == 'VARCHAR' or dfStats.data_type[i].upper() == 'TEXT':\n",
        "            fb = 'randstr(uniform(' + str(dfStats.min_length[i]) + ',' + str(dfStats.max_length[i]) + ', random(' + str(seed_1) + ')),uniform(1,'+ str(dfStats.cardinality[i]) + ',random(' + str(seed_1) + '))) '\n",
        "            fe = '::varchar(' + str(int(dfStats.data_type_length[i])) + ') '\n",
        "        elif dfStats.data_type[i].upper() == 'NUMBER':\n",
        "            if dfStats.cardinality[i] == dfStats.total_cardinality[0] and int(dfStats.data_type_precision[i]) == 0:\n",
        "                fb = '(seq8()+1)'\n",
        "            else:\n",
        "                fb = 'uniform(1,' + str(dfStats.cardinality[i]) + ' , random('+ str(seed_1) +')) '\n",
        "            fe = '::number(' + str(int(dfStats.data_type_length[i])) + ',' + str(int(dfStats.data_type_precision[i])) + ') '\n",
        "        elif dfStats.data_type[i].upper() == 'FLOAT':\n",
        "            fb = \"uniform(1,ceil(\" + str(int(dfStats.cardinality[i])/4) + \") , random(\"+ str(seed_1) +\")) || '.' || uniform(1,9999,random(\" + str(seed_1) + \")) \"\n",
        "            fe = '::float'\n",
        "        else:\n",
        "            fb = 'NULL '\n",
        "            fe = '::' + dfStats.data_type[i].upper()\n",
        "\n",
        "        if (fb == ''):\n",
        "            print( \"WARNING: Line: {0} Unknown Datatype: {1}\".format(i,dfStats.data_type[i]))\n",
        "        else:\n",
        "            if (int(dfStats.cardinality[i]) == 0):\n",
        "                formula = 'null' + fe\n",
        "            elif (dfStats.total_null[i] == 0):\n",
        "                formula = fb + fe\n",
        "            else:\n",
        "                formula = '(case when uniform(1,1000,random(' + str(seed_1+10000) + ')) <= (' + str(int(dfStats.total_null[i])) + '/' + str(dfStats.total_cardinality[0]) + ')*1000 then null else ' + fb +' end) ' + fe\n",
        "\n",
        "        if i == 0:\n",
        "            sqlQuery += formula\n",
        "        else:\n",
        "            sqlQuery += \"\"\",\n",
        "            \"\"\" + formula\n",
        "\n",
        "    sqlQuery += \"\"\"\n",
        "        FROM table(generator(rowcount => \"\"\" + str(dfStats.total_cardinality[0]) + \"\"\"));\"\"\"\n",
        "\n",
        "    return sqlQuery"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_PMvDA12ANti"
      },
      "source": [
        "## Create Source and Target SQLAlchemy Engines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku6ri2zljH2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SourceEngine = getEngine(snowUser=sourceUser, snowPassword=sourcePassword, snowAccount=sourceAccount, snowDatabase=sourceDatabase, snowSchema=sourceSchema, snowWarehouse=sourceWarehouse)\n",
        "TargetEngine = getEngine(snowUser=targetUser, snowPassword=targetPassword, snowAccount=targetAccount, snowDatabase=targetDatabase, snowSchema=targetSchema, snowWarehouse=targetWarehouse)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n07upO--AZ_e"
      },
      "source": [
        "## List Tables to Analyze from Source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJMRWJvtlFLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sqlQuery = \"\"\"\n",
        "    SHOW TABLES IN \"\"\" + sourceSchema + \"\"\";\n",
        "    \"\"\"\n",
        "\n",
        "dfTables = execSQLDataFrame(SourceEngine, sqlQuery)\n",
        "\n",
        "dfTables"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CsleHD4vAfxA"
      },
      "source": [
        "## Main Logic\n",
        "This section will iterate through the list of Tables, get key statistics on each table, generate the necessary SQL to create data on the target, and then insert that randomly generated data into the target table.\n",
        "\n",
        "##### *Note: This section will detect whether the tables exist on the target system.  If they do, they will insert (no truncate).  If they do not exist, the tables will be created.*\n",
        "\n",
        "\n",
        "##### *Also Note: If the target table definition is different, you may see errors or odd behavior.  If the source table has constraints, it is possible that the CREATE or REPLACE TABLE statement will fail.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-05Xao-QnHwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,row in dfTables.iterrows():\n",
        "    sqlQuery = \"\"\"\n",
        "        SELECT *\n",
        "        FROM information_schema.columns\n",
        "        WHERE table_name = '\"\"\" + str(dfTables.name[i]) + \"\"\"'\n",
        "        AND table_schema = '\"\"\" + sourceSchema + \"\"\"'\n",
        "        AND data_type NOT IN ('VARIANT','ARRAY')\n",
        "        ORDER BY ordinal_position;\n",
        "        \"\"\"\n",
        "    dfColumns = execSQLDataFrame(SourceEngine, sqlQuery)\n",
        "\n",
        "    if dfColumns.shape[0] != 0:\n",
        "        dfStats = execColStats(dfColumns,str(dfTables.rows[i]))\n",
        "\n",
        "        sqlQuery = execGenDataSQL(dfStats)\n",
        "\n",
        "        sqlCheckQuery = \"\"\"SELECT table_name FROM information_schema.tables WHERE table_name = '\"\"\" + dfTables.name[i] + \"\"\"' and table_catalog = '\"\"\" + targetDatabase + \"\"\"';\"\"\"\n",
        "        target_check = execSQLDataFrame(TargetEngine, sqlCheckQuery)\n",
        "\n",
        "        if target_check.shape[0] != 0:\n",
        "            output = execSQLDataFrame(TargetEngine, sqlQuery)\n",
        "            print(sqlQuery)\n",
        "        else:\n",
        "            target_ddl_sql = \"\"\"SELECT get_ddl('table','\"\"\" + dfTables.name[i] + \"\"\"') as ctas;\"\"\"\n",
        "            target_ddl = execSQLDataFrame(SourceEngine, target_ddl_sql)\n",
        "\n",
        "            execSQLDataFrame(TargetEngine, target_ddl.ctas[0])\n",
        "            output = execSQLDataFrame(TargetEngine, sqlQuery)\n",
        "            print(sqlQuery)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOmbnUd9wZzS",
        "colab_type": "text"
      },
      "source": [
        "## Optional Main Logic\n",
        "This section will iterate through the list of Tables, get key statistics on each table, generate the necessary SQL to create data on the target, and then print the resulting SQL to the Notebook, so it can be executed manually on Snowflake."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpEdqKUuFqsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,row in dfTables.iterrows():\n",
        "    sqlQuery = \"\"\"\n",
        "        SELECT *\n",
        "        FROM information_schema.columns\n",
        "        WHERE table_name = '\"\"\" + str(dfTables.name[i]) + \"\"\"'\n",
        "        AND table_schema = '\"\"\" + sourceSchema + \"\"\"'\n",
        "        AND data_type NOT IN ('VARIANT','ARRAY')\n",
        "        ORDER BY ordinal_position;\n",
        "        \"\"\"\n",
        "    dfColumns = execSQLDataFrame(SourceEngine, sqlQuery)\n",
        "\n",
        "    if dfColumns.shape[0] != 0:\n",
        "        dfStats = execColStats(dfColumns,str(dfTables.rows[i]))\n",
        "\n",
        "        sqlQuery = execGenDataSQL(dfStats)\n",
        "        \n",
        "        print(sqlQuery)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}